{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "\n",
    "# BCB dataset\n",
    "\n",
    "node_type_dict = {'MethodDeclaration' : 0, 'FormalParameter' : 1, 'LocalVariableDeclaration' : 2, 'VariableDeclarator' : 3, \n",
    "                  'BinaryOperation' : 4, 'IfStatement' : 5, 'BlockStatement' : 6, 'StatementExpression' : 7, 'Assignment' : 8, \n",
    "                  'MethodInvocation' : 9, 'ForStatement' : 10, 'TryStatement' : 11, 'ClassCreator' : 12, 'CatchClause' : 13, \n",
    "                  'WhileStatement' : 14, 'ReturnStatement' : 15, 'OtherStmt' : 16}\n",
    "\n",
    "stmt_list = node_type_dict.keys()\n",
    "model = Word2Vec.load(\"models/word2vec_model.bin\")\n",
    "\n",
    "def find_root_name(list):\n",
    "    root_name = None\n",
    "    for i in list:\n",
    "        if i in stmt_list:\n",
    "            root_name = i\n",
    "            break\n",
    "    \n",
    "    if root_name is None:\n",
    "        root_name = 'OtherStmt'\n",
    "    return root_name\n",
    "\n",
    "\n",
    "def get_stmt_sequence(node_sequence):\n",
    "    stmt_index = []\n",
    "    word_bag = []\n",
    "    \n",
    "    for i in range(len(node_sequence)):\n",
    "        if node_sequence[i] in stmt_list:\n",
    "            stmt_index.append(i)\n",
    "    \n",
    "    pairs = [[stmt_index[i], stmt_index[i+1]] for i in range(len(stmt_index)-1)]\n",
    "    if len(stmt_index) >= 2:\n",
    "        if pairs[-1][1] != len(node_sequence):\n",
    "            pairs.append([pairs[-1][1], len(node_sequence)])\n",
    "        for i in range(len(pairs)):\n",
    "            if pairs[i][1] - pairs[i][0] <= 2 and i != len(pairs) - 1:\n",
    "                pairs[i+1][0] = pairs[i][0]\n",
    "            else:\n",
    "                word_bag.append(node_sequence[pairs[i][0]:pairs[i][1]])\n",
    "    else:\n",
    "        word_bag.append(node_sequence)\n",
    "    \n",
    "    return word_bag\n",
    "\n",
    "\n",
    "def compute_Embedding(path):\n",
    "    with open(path, 'rb') as file_1:\n",
    "        node_dict = pickle.load(file_1)\n",
    "\n",
    "#     embeddings = torch.zeros(17, 100)\n",
    "    embeddings = torch.zeros(58, 100)\n",
    "\n",
    "    for k, v in node_dict.items():\n",
    "        node_sequence = v[1]\n",
    "        centrality = v[2]  # 该社区的中心性\n",
    "        word_bag = get_stmt_sequence(node_sequence)\n",
    "        for sequence in word_bag:\n",
    "            root_name = find_root_name(sequence) # 子树根节点的名称\n",
    "          \n",
    "            index = node_type_dict[root_name]  # 在嵌入维度中的行索引\n",
    "\n",
    "            embedding = np.mean([model.wv[word] for word in sequence], axis=0)\n",
    "\n",
    "            embedding = torch.tensor(embedding)\n",
    "            embedding = embedding * centrality\n",
    "\n",
    "            embeddings[index] = embeddings[index] + embedding\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_path = \"processed_data/node_pkl_subgraph/\"\n",
    "#     output_path = \"processed_data/embeddings/\"\n",
    "    output_path = \"processed_data/embeddings_3/\"\n",
    "\n",
    "    files = os.listdir(input_path)\n",
    "    in_files = os.listdir(output_path)\n",
    "    process_file = []\n",
    "    print(len(files))\n",
    "    for i in files:\n",
    "        result = re.findall(r'\\d+\\w', i)\n",
    "        if result[0] + '.pt' not in in_files:\n",
    "            process_file.append(i)\n",
    "    \n",
    "#     print(process_file)\n",
    "    i = 0\n",
    "    for index, file in tqdm(enumerate(process_file), total=len(process_file), desc=\"Processing\"):\n",
    "        embeddings = compute_Embedding(input_path + file)\n",
    "        \n",
    "        a = embeddings.shape\n",
    "        if a[0] == 16:\n",
    "            i += 1\n",
    "        if index == 0:\n",
    "            print(a[0])\n",
    "        file_name = re.findall(r'\\d+\\w', file)\n",
    "        torch.save(embeddings, output_path + file_name[0] + '.pt')\n",
    "    \n",
    "    print(i)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "\n",
    "# GCJ dataset\n",
    "\n",
    "node_type_dict = {'MethodDeclaration' : 0, 'FormalParameter' : 1, 'LocalVariableDeclaration' : 2, 'VariableDeclarator' : 3, \n",
    "                  'BinaryOperation' : 4, 'IfStatement' : 5, 'BlockStatement' : 6, 'StatementExpression' : 7, 'Assignment' : 8, \n",
    "                  'MethodInvocation' : 9, 'ForStatement' : 10, 'TryStatement' : 11, 'ClassCreator' : 12, 'CatchClause' : 13, \n",
    "                  'WhileStatement' : 14, 'ReturnStatement' : 15, 'OtherStmt' : 16}\n",
    "\n",
    "stmt_list = node_type_dict.keys()\n",
    "model = Word2Vec.load(\"models/word2vec_model_gcj.bin\")\n",
    "\n",
    "def find_root_name(list):\n",
    "    root_name = None\n",
    "    for i in list:\n",
    "        if i in stmt_list:\n",
    "            root_name = i\n",
    "            break\n",
    "    \n",
    "    if root_name is None:\n",
    "        root_name = 'OtherStmt'\n",
    "    return root_name\n",
    "\n",
    "\n",
    "def get_stmt_sequence(node_sequence):\n",
    "    stmt_index = []\n",
    "    word_bag = []\n",
    "    \n",
    "    for i in range(len(node_sequence)):\n",
    "        if node_sequence[i] in stmt_list:\n",
    "            stmt_index.append(i)\n",
    "    \n",
    "    pairs = [[stmt_index[i], stmt_index[i+1]] for i in range(len(stmt_index)-1)]\n",
    "    if len(stmt_index) >= 2:\n",
    "        if pairs[-1][1] != len(node_sequence):\n",
    "            pairs.append([pairs[-1][1], len(node_sequence)])\n",
    "        for i in range(len(pairs)):\n",
    "            if pairs[i][1] - pairs[i][0] <= 2 and i != len(pairs) - 1:\n",
    "                pairs[i+1][0] = pairs[i][0]\n",
    "            else:\n",
    "                word_bag.append(node_sequence[pairs[i][0]:pairs[i][1]])\n",
    "    else:\n",
    "        word_bag.append(node_sequence)\n",
    "    \n",
    "    return word_bag\n",
    "\n",
    "\n",
    "def compute_Embedding(path):\n",
    "    with open(path, 'rb') as file_1:\n",
    "        node_dict = pickle.load(file_1)\n",
    "\n",
    "    embeddings = torch.zeros(17, 100)\n",
    "\n",
    "    for k, v in node_dict.items():\n",
    "        node_sequence = v[1]\n",
    "        centrality = v[2]  # 该社区的中心性\n",
    "        word_bag = get_stmt_sequence(node_sequence)\n",
    "        for sequence in word_bag:\n",
    "            root_name = find_root_name(sequence) # 子树根节点的名称\n",
    "          \n",
    "            index = node_type_dict[root_name]  # 在嵌入维度中的行索引\n",
    "\n",
    "#             embedding = np.mean([model.wv[word] for word in sequence], axis=0)\n",
    "            embedding = np.sum([model.wv[word] for word in sequence], axis=0)\n",
    "\n",
    "            embedding = torch.tensor(embedding)\n",
    "            embedding = embedding * centrality\n",
    "\n",
    "            embeddings[index] = embeddings[index] + embedding\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def main():\n",
    "    for s in range(12):\n",
    "        input_path = \"processed_data/GCJ/GCJ_json/\" + str(s+1) + '/'\n",
    "    #     output_path = \"processed_data/embeddings/\"\n",
    "        output_path = \"processed_data/GCJ/embeddings_1/googlejam4_src/\" + str(s+1) + '/'\n",
    "\n",
    "        files = os.listdir(input_path)\n",
    "\n",
    "        i = 0\n",
    "        for index, file in tqdm(enumerate(files), total=len(files), desc=\"Processing\"):\n",
    "            embeddings = compute_Embedding(input_path + file)\n",
    "\n",
    "            a = embeddings.shape\n",
    "#             file_name = re.findall(r'\\d+\\w', file)\n",
    "            torch.save(embeddings, output_path + file + '.pt')\n",
    "\n",
    "        print(i)\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
