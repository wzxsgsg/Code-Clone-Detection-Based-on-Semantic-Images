{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import psutil\n",
    "import argparse\n",
    "from CNN1 import SimpleCNN1D\n",
    "# from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "# 测试10折交叉验证 （分层）\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--datasets\", default='BCB')\n",
    "parser.add_argument(\"--data_setting\", default='11')\n",
    "parser.add_argument(\"--batch_size\", default=32)\n",
    "parser.add_argument(\"--epochs\", default=5)\n",
    "parser.add_argument(\"--lr\", default=0.001)\n",
    "parser.add_argument(\"--threshold\", default=0.7)\n",
    "parser.add_argument(\"--weight_decay\", default=0.0001)\n",
    "parser.add_argument(\"--input_channels\", default=100)\n",
    "parser.add_argument(\"--out_channels_1\", default=64)\n",
    "parser.add_argument(\"--out_channels_2\", default=32)\n",
    "parser.add_argument(\"--hidden_size_1\", default=32)\n",
    "parser.add_argument(\"--hidden_size_2\", default=16)\n",
    "parser.add_argument(\"--output_size\", default=1)\n",
    "parser.add_argument(\"--n_splits\", default=7)\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "mem_usage = []\n",
    "gpu_usage = []\n",
    "\n",
    "model = SimpleCNN1D(args.input_channels, args.out_channels_1, args.out_channels_2, args.hidden_size_1, args.hidden_size_2, args.output_size).to(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "\n",
    "def create_batches(data):\n",
    "    # random.shuffle(data)\n",
    "    batches = [data[i:i + args.batch_size] for i in range(0, len(data), args.batch_size)]\n",
    "    return batches\n",
    "\n",
    "def get_dataset(id):\n",
    "    indexdir = '../input/codeclone_bcb/BCB/'\n",
    "    if id == '0':\n",
    "        trainfile = open(indexdir + 'traindata.txt')\n",
    "        validfile = open(indexdir + 'devdata.txt')\n",
    "        testfile = open(indexdir + 'testdata.txt')\n",
    "    elif id == '11':\n",
    "        trainfile = open(indexdir + 'traindata11.txt')\n",
    "        validfile = open(indexdir + 'devdata.txt')\n",
    "        testfile = open(indexdir + 'testdata.txt')\n",
    "    else:\n",
    "        print('file not exist')\n",
    "        quit()\n",
    "    trainlist = trainfile.readlines()\n",
    "    print(len(trainlist))\n",
    "    validlist = validfile.readlines()\n",
    "    print(len(validlist))\n",
    "    testlist = testfile.readlines()\n",
    "    print(len(testlist))\n",
    "    print(\"data total number:\", len(trainlist)+len(validlist)+len(testlist))\n",
    "    \n",
    "    return trainlist, validlist, testlist\n",
    "\n",
    "\n",
    "def gcj_dataset(id):\n",
    "    indexdir='../input/codeclone_gcj/javadata/'\n",
    "    if id=='0':\n",
    "        trainfile=open(indexdir+'trainall.txt')\n",
    "        validfile = open(indexdir+'valid.txt')\n",
    "        testfile = open(indexdir+'test.txt')\n",
    "    elif id=='13':\n",
    "        trainfile = open(indexdir+'train13.txt')\n",
    "        validfile = open(indexdir+'valid.txt')\n",
    "        testfile = open(indexdir+'test.txt')\n",
    "    elif id=='11':\n",
    "        trainfile = open(indexdir+'train11.txt')\n",
    "        validfile = open(indexdir+'valid.txt')\n",
    "        testfile = open(indexdir+'test.txt')\n",
    "    elif id=='0small':\n",
    "        trainfile = open(indexdir+'trainsmall.txt')\n",
    "        validfile = open(indexdir+'valid.txt')\n",
    "        testfile = open(indexdir+'test.txt')\n",
    "    elif id == '13small':\n",
    "        trainfile = open(indexdir+'train13small.txt')\n",
    "        validfile = open(indexdir+'validsmall.txt')\n",
    "        testfile = open(indexdir+'testsmall.txt')\n",
    "    elif id=='11small':\n",
    "        trainfile = open(indexdir+'train11small.txt')\n",
    "        validfile = open(indexdir+'validsmall.txt')\n",
    "        testfile = open(indexdir+'testsmall.txt')\n",
    "    else:\n",
    "        print('file not exist')\n",
    "        quit()\n",
    "    trainlist=trainfile.readlines()\n",
    "    print(len(trainlist))\n",
    "    validlist=validfile.readlines()\n",
    "    print(len(validlist))\n",
    "    testlist=testfile.readlines()\n",
    "    print(len(testlist))\n",
    "    \n",
    "    return trainlist, validlist, testlist\n",
    "\n",
    "\n",
    "def train_and_valid(trainlist, y, file_path, epochs):\n",
    "    epochs = trange(epochs, leave=True, desc=\"Epoch\")\n",
    "    best_f1 = None\n",
    "    total_train_time = 0\n",
    "#     total_valid_time = 0\n",
    "    final_loss = 0\n",
    "    count = 0\n",
    "    for epoch in epochs:\n",
    "        batches = create_batches(trainlist)\n",
    "        totalloss = 0.0\n",
    "        main_index = 0.0\n",
    "        train_time_start = time.time()\n",
    "        for index, batch in tqdm(enumerate(batches), total=len(batches), desc=\"Batches\"):\n",
    "            optimizer.zero_grad()\n",
    "            batchloss = 0\n",
    "            for index in batch:\n",
    "                code1path = '../input/codeclone_bcb/BCB' + file_path[index][0].strip('.')\n",
    "                code2path = '../input/codeclone_bcb/BCB' + file_path[index][1].strip('.')\n",
    "#                 label = int(file_path[index][2])\n",
    "                label = int(y[index])\n",
    "                embeddings_dir = 'processed_data/embeddings_3/'  # embeddings_3是词向量序列嵌入，4是随机游走嵌入\n",
    "\n",
    "                result_1 = re.findall(r'\\d+\\w', code1path)\n",
    "                result_2 = re.findall(r'\\d+\\w', code2path)\n",
    "\n",
    "                code1_embedding_path = embeddings_dir + result_1[0] + '.pt'\n",
    "                code2_embedding_path = embeddings_dir + result_2[0] + '.pt'\n",
    "                \n",
    "#                 code1path = file_path[index][0]\n",
    "#                 code2path = file_path[index][1] # 此处的注释为GCJ数据集\n",
    "#                 label = int(y[index])\n",
    "        \n",
    "#                 embeddings_dir = 'processed_data/GCJ/embeddings_1/' # embeddings_1是词向量嵌入，2是随机游走嵌入\n",
    "        \n",
    "#                 code1_embedding_path = embeddings_dir + code1path + '.pkl.pt'\n",
    "#                 code2_embedding_path = embeddings_dir + code2path + '.pkl.pt'\n",
    "                \n",
    "                code1_feature = torch.load(code1_embedding_path).to(device)\n",
    "                code2_feature = torch.load(code2_embedding_path).to(device)\n",
    "                \n",
    "                if label == -1:\n",
    "                    label = 0\n",
    "                    \n",
    "                label = torch.tensor(label, dtype=torch.float, device=device)\n",
    "                label = torch.unsqueeze(label, 0)\n",
    "#                 vector = torch.tensor(vector, device=device)\n",
    "                \n",
    "                output = model(code1_feature, code2_feature)\n",
    "                output = torch.squeeze(output, 0)\n",
    "\n",
    "                mem = psutil.virtual_memory()\n",
    "                mem_usage.append(mem.used) # 记录内存的使用情况\n",
    "\n",
    "                gpu_usage.append(torch.cuda.memory_allocated()) # 记录显存的使用情况\n",
    "\n",
    "#                 print(output)\n",
    "                batchloss = batchloss + criterion(output, label)\n",
    "                \n",
    "#             batchloss.backward(retain_graph=True)\n",
    "            batchloss.backward()\n",
    "            optimizer.step()\n",
    "            loss = batchloss.item()\n",
    "            totalloss += loss\n",
    "            main_index = main_index + len(batch)\n",
    "            loss = totalloss / main_index\n",
    "            epochs.set_description(\"Epoch (Loss=%g)\" % round(loss, 5))\n",
    "\n",
    "        train_time_end = time.time()\n",
    "        train_time = train_time_end - train_time_start\n",
    "        total_train_time = total_train_time + train_time\n",
    "        \n",
    "        final_loss = totalloss / main_index\n",
    "        \n",
    "    avg_mem = sum(mem_usage) / len(mem_usage)\n",
    "    avg_gpu = sum(gpu_usage) / len(gpu_usage) # 每训练一个完整周期所平均使用的显存与内存\n",
    "    \n",
    "    return total_train_time, final_loss, avg_mem, avg_gpu\n",
    "\n",
    "\n",
    "def test(testlist, y, file_path):\n",
    "    # model.eval()\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    results = []\n",
    "    for index in testlist:\n",
    "        code1path = '../input/codeclone_bcb/BCB' + file_path[index][0].strip('.')\n",
    "        code2path = '../input/codeclone_bcb/BCB' + file_path[index][1].strip('.')\n",
    "#         label = int(file_path[index][2])\n",
    "        label = int(y[index])\n",
    "        embeddings_dir = 'processed_data/embeddings_3/'\n",
    "\n",
    "        result_1 = re.findall(r'\\d+\\w', code1path)\n",
    "        result_2 = re.findall(r'\\d+\\w', code2path)\n",
    "\n",
    "        code1_embedding_path = embeddings_dir + result_1[0] + '.pt'\n",
    "        code2_embedding_path = embeddings_dir + result_2[0] + '.pt'\n",
    "        \n",
    "#         code1path = file_path[index][0]\n",
    "#         code2path = file_path[index][1] # 此处的注释为GCJ数据集\n",
    "#         label = int(y[index])\n",
    "        \n",
    "#         embeddings_dir = 'processed_data/GCJ/embeddings_1/' # embeddings_1是词向量嵌入，2是随机游走嵌入\n",
    "        \n",
    "#         code1_embedding_path = embeddings_dir + code1path + '.pkl.pt'\n",
    "#         code2_embedding_path = embeddings_dir + code2path + '.pkl.pt'\n",
    "\n",
    "        code1_feature = torch.load(code1_embedding_path).to(device)\n",
    "        code2_feature = torch.load(code2_embedding_path).to(device)\n",
    "        \n",
    "        if label == -1:\n",
    "            label = 0\n",
    "                    \n",
    "        label = torch.tensor(label, dtype=torch.float, device=device)\n",
    "        \n",
    "        output = model(code1_feature, code2_feature)\n",
    "        output = torch.squeeze(output, 0)\n",
    "        results.append(output.item())\n",
    "        prediction = output.item()\n",
    "\n",
    "        if prediction > args.threshold and label.item() == 1:\n",
    "            tp += 1\n",
    "            # print('tp')\n",
    "        if prediction <= args.threshold and label.item() == 0:\n",
    "            tn += 1\n",
    "            # print('tn')\n",
    "        if prediction > args.threshold and label.item() == 0:\n",
    "            fp += 1\n",
    "            # print('fp')\n",
    "        if prediction <= args.threshold and label.item() == 1:\n",
    "            fn += 1\n",
    "            # print('fn')\n",
    "    print(tp, tn, fp, fn)\n",
    "    p = 0.0\n",
    "    r = 0.0\n",
    "    f1 = 0.0\n",
    "    if tp + fp == 0:\n",
    "        print('precision is none')\n",
    "        return\n",
    "    p = tp / (tp + fp)\n",
    "    if tp + fn == 0:\n",
    "        print('recall is none')\n",
    "        return\n",
    "    r = tp / (tp + fn)\n",
    "    f1 = 2 * p * r / (p + r)\n",
    "    print('precision')\n",
    "    print(p)\n",
    "    print('recall')\n",
    "    print(r)\n",
    "    print('F1')\n",
    "    print(f1)\n",
    "    return results, p, r, f1, tp, tn, fp, fn\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=args.n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    trainlist = []\n",
    "    validlist = []\n",
    "    testlist = []\n",
    "    \n",
    "    total_train_time = 0\n",
    "    total_test_time = 0\n",
    "    fold_p_scores = []\n",
    "    fold_r_scores = []\n",
    "    fold_f1_scores = []\n",
    "    fold_avg_mem = []\n",
    "    fold_avg_gpu = []\n",
    "    fold_classification_statistics = {}\n",
    "\n",
    "    if args.datasets == 'BCB':\n",
    "        trainlist, validlist, testlist = get_dataset(args.data_setting) #BCB\n",
    "        print(\"Using dataset: BCB\")\n",
    "    else:\n",
    "        trainlist, validlist, testlist = gcj_dataset(args.data_setting) #GCJ\n",
    "        print(\"Using dataset: GCJ\")\n",
    "    \n",
    "    full_data = trainlist + validlist + testlist\n",
    "    \n",
    "    file_path = []\n",
    "    labels = []\n",
    "    \n",
    "    for line in full_data:\n",
    "        pairinfo = line.split()\n",
    "        code1path = pairinfo[0].replace('\\\\', '/')\n",
    "        code2path = pairinfo[1].replace('\\\\', '/')\n",
    "        label=int(pairinfo[2])\n",
    "\n",
    "#         file_path.append([code1path, code2path, label])\n",
    "        file_path.append([code1path, code2path])\n",
    "        labels.append(label)\n",
    "    \n",
    "    y = np.array(labels)\n",
    "    k = 0\n",
    "    train_data_len = 0\n",
    "    test_data_len = 0\n",
    "#     for train_index, test_index in kf.split(file_path):\n",
    "    for train_index, test_index in skf.split(file_path, y):\n",
    "        k += 1\n",
    "        print(f'第{k}折交叉验证开始:')\n",
    "        print('train data:', len(train_index))\n",
    "        print('test data:', len(test_index))\n",
    "        \n",
    "        train_data_len = len(train_index)\n",
    "        test_data_len = len(test_index)\n",
    "        \n",
    "        train_time, loss, avg_mem, avg_gpu = train_and_valid(train_index, y, file_path, args.epochs)\n",
    "        \n",
    "        test_time_start = time.time()\n",
    "        testresults, test_p, test_r, test_f1, test_tp, test_tn, test_fp, test_fn = test(test_index, y, file_path)\n",
    "        test_time_end = time.time()\n",
    "        test_time = test_time_end - test_time_start\n",
    "        \n",
    "        fold_p_scores.append(test_p)\n",
    "        fold_r_scores.append(test_r)\n",
    "        fold_f1_scores.append(test_f1)\n",
    "        fold_avg_mem.append(avg_mem)\n",
    "        fold_avg_gpu.append(avg_gpu)\n",
    "        fold_classification_statistics[f'{k}_fold: '] = [test_tp, test_tn, test_fp, test_fn]\n",
    "        total_train_time = total_train_time + train_time\n",
    "        total_test_time = total_test_time + test_time\n",
    "    \n",
    "    print()\n",
    "    print(\"每折交叉验证训练数据数量：\", train_data_len)\n",
    "    print(\"每折交叉验证验证数据数量：\", test_data_len)\n",
    "    \n",
    "    print(f\"{args.n_splits}折交叉验证总训练时间:\", total_train_time)\n",
    "    print(f\"{args.n_splits}折交叉验证总测试时间:\", total_test_time)\n",
    "    \n",
    "    print(\"每折的Precision分数:\", fold_p_scores)\n",
    "    print(\"每折的Recall分数:\", fold_r_scores)\n",
    "    print(\"每折的F1分数:\", fold_f1_scores)\n",
    "    \n",
    "    print(\"Precision平均分数:\", np.mean(fold_p_scores))\n",
    "    print(\"Recall平均分数:\", np.mean(fold_r_scores))\n",
    "    print(\"F1平均分数:\", np.mean(fold_f1_scores))\n",
    "    print(\"平均内存使用情况:\", np.mean(fold_avg_mem))\n",
    "    print(\"平均显存使用情况:\", np.mean(fold_avg_gpu))\n",
    "    \n",
    "    finalfile = open('result/BCB_n_fold_cross_validation', mode='w')\n",
    "#     finalfile = open('result/GCJ_n_fold_cross_validation', mode='w')\n",
    "    finalfile.write('使用的数据集: ' + str(args.datasets) + '\\n')\n",
    "    finalfile.write('threshold: ' + str(args.threshold) + '\\n')\n",
    "    finalfile.write(f\"{args.n_splits}_fold_train_time:\" + str(total_train_time) + '\\n')\n",
    "    finalfile.write(f\"{args.n_splits}_fold_test_time:\" + str(total_test_time) + '\\n')\n",
    "    finalfile.write(\"每折交叉验证训练数据数量:\" + str(train_data_len) + '\\n')\n",
    "    finalfile.write(\"每折交叉验证验证数据数量:\" + str(test_data_len) + '\\n')\n",
    "    finalfile.write(\"每折交叉验证的结果:\" + str(fold_classification_statistics) + '\\n')\n",
    "    finalfile.write('每折的Precision分数:' + str(fold_p_scores) + '\\n')\n",
    "    finalfile.write('每折的Recall分数:' + str(fold_r_scores) + '\\n')\n",
    "    finalfile.write('每折的F1分数:' + str(fold_f1_scores) + '\\n')\n",
    "    finalfile.write('Precision平均分数:' + str(np.mean(fold_p_scores)) + '\\n')\n",
    "    finalfile.write('Recall平均分数:' + str(np.mean(fold_r_scores)) + '\\n')\n",
    "    finalfile.write('F1平均分数:' + str(np.mean(fold_f1_scores)) + '\\n')\n",
    "    finalfile.write('平均内存使用情况:' + str(np.mean(fold_avg_mem) / (1024 ** 2)) + 'MB' + '\\n')\n",
    "    finalfile.write('平均显存使用情况:' + str(np.mean(fold_avg_gpu) / (1024 ** 2)) + 'MB' + '\\n')\n",
    "    finalfile.close()\n",
    "    \n",
    "    \n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
