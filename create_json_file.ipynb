{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9134\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ed4bdde2064023ac394ad01feb7bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/9134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.ipynb_checkpoints']\n"
     ]
    }
   ],
   "source": [
    "from community import community_louvain\n",
    "import networkx as nx\n",
    "import re\n",
    "import os\n",
    "from create_ast_graph import create_ast_graph_func\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# BCB\n",
    "\n",
    "def louvain_detect(g, x):\n",
    "    partition = community_louvain.best_partition(g)  # 社区检测\n",
    "    pvq = list(set(partition.values()))\n",
    "    #     print(\"pvq:\", pvq)\n",
    "    #     print(\"x_length:\", len(x))\n",
    "    val_dict = {}  # 每个分区所对应的token\n",
    "    val_dict_num = {}  # 每个分区所对应token的序号\n",
    "    for key in partition.keys():  # 求每个区间所含有的token\n",
    "        for i in pvq:\n",
    "            if partition[key] == i:\n",
    "                if i in val_dict.keys():\n",
    "                    val_dict[i].append(x[key])\n",
    "                    val_dict_num[i].append(key)\n",
    "                else:\n",
    "                    val_dict[i] = [x[key]]\n",
    "                    val_dict_num[i] = [key]\n",
    "\n",
    "    return partition, val_dict, val_dict_num\n",
    "\n",
    "\n",
    "def get_subgraph_centrality(partition, g):\n",
    "    G_new = nx.Graph()\n",
    "    for u, v in g.edges():\n",
    "        u_part = partition[u]\n",
    "        v_part = partition[v]\n",
    "        if u_part != v_part and not G_new.has_edge(u_part, v_part):\n",
    "            G_new.add_edge(u_part, v_part)\n",
    "\n",
    "    subgraph_degree_centrality = list(nx.degree_centrality(G_new).values())\n",
    "    subgraph_betweenness_centrality = list(nx.betweenness_centrality(G_new).values())\n",
    "    subgraph_closeness_centrality = list(nx.closeness_centrality(G_new).values())\n",
    "    subgraph_harmonic_centrality = list(nx.harmonic_centrality(G_new).values())\n",
    "    subgraph_centrality_result = ((np.array(subgraph_degree_centrality) + np.array(\n",
    "        subgraph_betweenness_centrality) + np.array(subgraph_closeness_centrality) + np.array(\n",
    "        subgraph_harmonic_centrality)) / 4).tolist()\n",
    "\n",
    "    subgraph_result = dict(zip(G_new.nodes(), subgraph_centrality_result))\n",
    "    return subgraph_result\n",
    "\n",
    "# 得到每个分区的子图，以及子图的结点和边\n",
    "def get_subgraph(partition, g, x):\n",
    "    subgraphs = {}\n",
    "    for node, comm in partition.items():\n",
    "        if comm not in subgraphs:\n",
    "            subgraphs[comm] = nx.Graph()\n",
    "        subgraphs[comm].add_node(node)\n",
    "\n",
    "    for u, v in g.edges():\n",
    "        if partition[u] == partition[v]:\n",
    "            subgraphs[partition[u]].add_edge(u, v)\n",
    "\n",
    "    # for i, subgraph in enumerate(subgraphs.values()):\n",
    "    #     nodes_name = []\n",
    "    #     print(f\"Subgraph {i+1} degree centrality: {sum(dict(nx.degree(subgraph)).values())}\")\n",
    "    #     # print(nx.degree_centrality(subgraph))\n",
    "    #     print(\"Nodes:\", list(subgraph.nodes()))\n",
    "    #     for j in list(subgraph.nodes()):\n",
    "    #         nodes_name.append(x[j])\n",
    "    #     print(\"Nodes_Name\", nodes_name)\n",
    "    #     print(\"Edges:\", list(subgraph.edges()))\n",
    "    #     print()\n",
    "\n",
    "    return subgraphs\n",
    "\n",
    "def get_nodes_dict(subgraphs, subgraph_result, x):\n",
    "    nodes_dict = {}\n",
    "    for i, subgraph_dict in enumerate(subgraphs.items()):\n",
    "        subgraph_key = subgraph_dict[0]\n",
    "        subgraph = subgraph_dict[1]\n",
    "        nodes_name = []\n",
    "        for j in list(subgraph.nodes()):\n",
    "            nodes_name.append(x[j])\n",
    "        nodes_dict[i] = [subgraph, nodes_name, subgraph_result[subgraph_key]]\n",
    "\n",
    "    return nodes_dict\n",
    "\n",
    "def create_json_file(filePath, outputPath):\n",
    "    g, alltokens, x, g_edge = create_ast_graph_func(filePath)  # 得到图、token、结点列表、边列表\n",
    "    partition, val_dict, val_dict_num = louvain_detect(g, x)\n",
    "    subgraph_result = get_subgraph_centrality(partition, g)\n",
    "    subgraphs = get_subgraph(partition, g, x)\n",
    "    nodes_dict = get_nodes_dict(subgraphs, subgraph_result, x)\n",
    "\n",
    "    with open(outputPath, 'wb') as file:\n",
    "#         json.dump(nodes_dict, file)\n",
    "        pickle.dump(nodes_dict, file)\n",
    "\n",
    "def main():\n",
    "    folder_path = \"../input/codeclone_bcb/BCB/bigclonebenchdata\"\n",
    "    outputPath = \"processed_data/node_pkl_subgraph/\"\n",
    "    files = os.listdir(folder_path)\n",
    "    print(len(files))\n",
    "    error_file = []\n",
    "    count = 0\n",
    "    print(\"开始处理：\")\n",
    "    for index, file in tqdm(enumerate(files), total=len(files), desc=\"Processing\"):\n",
    "        file_name = re.findall(r'\\d+\\w', file)\n",
    "#         print(file_name)\n",
    "        try:\n",
    "            create_json_file(folder_path+'/'+file, outputPath+file_name[0]+'.pkl')\n",
    "        except:\n",
    "            error_file.append(file)\n",
    "            continue\n",
    "        \n",
    "        count += 1\n",
    "        if count % 10000 == 0:\n",
    "            print(\"已处理：\", count)\n",
    "    \n",
    "    print(error_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72278261a1db44aa86f4b4b0ecb3f44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "88\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a09338a6ef470d8be32760337e2002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "242\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e215e299b3fb44e8b6ae9e7db8e59312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "38\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e109fb0a01471c86c881f2ec4314f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "2\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "489313b0cee144f68396b028d1967788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "435\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ca07d372b34c8ea2100da9d1c55beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "27\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "698ef92857d54011aa0bf9a221c1db71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "245\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e31cd2b17849e4b847ef6a2c3b29c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "68\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c102b727214721a3188cabe763f0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "18\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cbac8691c1a4a1c963b33ab2eac24f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "20\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d055c944944293a8f5c57c428a7b94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "4\n",
      "开始处理：\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b8eb922166454496912b0aefd34117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from community import community_louvain\n",
    "import networkx as nx\n",
    "import re\n",
    "import os\n",
    "from create_ast_graph import create_ast_graph_func\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# GCJ\n",
    "\n",
    "def louvain_detect(g, x):\n",
    "    partition = community_louvain.best_partition(g)  # 社区检测\n",
    "    pvq = list(set(partition.values()))\n",
    "    #     print(\"pvq:\", pvq)\n",
    "    #     print(\"x_length:\", len(x))\n",
    "    val_dict = {}  # 每个分区所对应的token\n",
    "    val_dict_num = {}  # 每个分区所对应token的序号\n",
    "    for key in partition.keys():  # 求每个区间所含有的token\n",
    "        for i in pvq:\n",
    "            if partition[key] == i:\n",
    "                if i in val_dict.keys():\n",
    "                    val_dict[i].append(x[key])\n",
    "                    val_dict_num[i].append(key)\n",
    "                else:\n",
    "                    val_dict[i] = [x[key]]\n",
    "                    val_dict_num[i] = [key]\n",
    "\n",
    "    return partition, val_dict, val_dict_num\n",
    "\n",
    "\n",
    "def get_subgraph_centrality(partition, g):\n",
    "    G_new = nx.Graph()\n",
    "    for u, v in g.edges():\n",
    "        u_part = partition[u]\n",
    "        v_part = partition[v]\n",
    "        if u_part != v_part and not G_new.has_edge(u_part, v_part):\n",
    "            G_new.add_edge(u_part, v_part)\n",
    "\n",
    "    subgraph_degree_centrality = list(nx.degree_centrality(G_new).values())\n",
    "    subgraph_betweenness_centrality = list(nx.betweenness_centrality(G_new).values())\n",
    "    subgraph_closeness_centrality = list(nx.closeness_centrality(G_new).values())\n",
    "    subgraph_harmonic_centrality = list(nx.harmonic_centrality(G_new).values())\n",
    "    subgraph_centrality_result = ((np.array(subgraph_degree_centrality) + np.array(\n",
    "        subgraph_betweenness_centrality) + np.array(subgraph_closeness_centrality) + np.array(\n",
    "        subgraph_harmonic_centrality)) / 4).tolist()\n",
    "\n",
    "    subgraph_result = dict(zip(G_new.nodes(), subgraph_centrality_result))\n",
    "    return subgraph_result\n",
    "\n",
    "# 得到每个分区的子图，以及子图的结点和边\n",
    "def get_subgraph(partition, g, x):\n",
    "    subgraphs = {}\n",
    "    for node, comm in partition.items():\n",
    "        if comm not in subgraphs:\n",
    "            subgraphs[comm] = nx.Graph()\n",
    "        subgraphs[comm].add_node(node)\n",
    "\n",
    "    for u, v in g.edges():\n",
    "        if partition[u] == partition[v]:\n",
    "            subgraphs[partition[u]].add_edge(u, v)\n",
    "\n",
    "    # for i, subgraph in enumerate(subgraphs.values()):\n",
    "    #     nodes_name = []\n",
    "    #     print(f\"Subgraph {i+1} degree centrality: {sum(dict(nx.degree(subgraph)).values())}\")\n",
    "    #     # print(nx.degree_centrality(subgraph))\n",
    "    #     print(\"Nodes:\", list(subgraph.nodes()))\n",
    "    #     for j in list(subgraph.nodes()):\n",
    "    #         nodes_name.append(x[j])\n",
    "    #     print(\"Nodes_Name\", nodes_name)\n",
    "    #     print(\"Edges:\", list(subgraph.edges()))\n",
    "    #     print()\n",
    "\n",
    "    return subgraphs\n",
    "\n",
    "def get_nodes_dict(subgraphs, subgraph_result, x):\n",
    "    nodes_dict = {}\n",
    "    for i, subgraph_dict in enumerate(subgraphs.items()):\n",
    "        subgraph_key = subgraph_dict[0]\n",
    "        subgraph = subgraph_dict[1]\n",
    "        nodes_name = []\n",
    "        for j in list(subgraph.nodes()):\n",
    "            nodes_name.append(x[j])\n",
    "        nodes_dict[i] = [subgraph, nodes_name, subgraph_result[subgraph_key]]\n",
    "\n",
    "    return nodes_dict\n",
    "\n",
    "def create_json_file(filePath, outputPath):\n",
    "    g, alltokens, x, g_edge = create_ast_graph_func(filePath)  # 得到图、token、结点列表、边列表\n",
    "    partition, val_dict, val_dict_num = louvain_detect(g, x)\n",
    "    subgraph_result = get_subgraph_centrality(partition, g)\n",
    "    subgraphs = get_subgraph(partition, g, x)\n",
    "    nodes_dict = get_nodes_dict(subgraphs, subgraph_result, x)\n",
    "\n",
    "    with open(outputPath, 'wb') as file:\n",
    "#         json.dump(nodes_dict, file)\n",
    "        pickle.dump(nodes_dict, file)\n",
    "\n",
    "    \n",
    "def main():\n",
    "    for i in range(12):\n",
    "        folder_path = '../input/codeclone_gcj/googlejam4_src/' + str(i+1) + '/'\n",
    "        outputPath = \"processed_data/GCJ/GCJ_json/\" + str(i+1) + '/'\n",
    "        files = os.listdir(folder_path)\n",
    "        print(len(files))\n",
    "        error_file = []\n",
    "        count = 0\n",
    "        print(\"开始处理：\")\n",
    "        for index, file in tqdm(enumerate(files), total=len(files), desc=\"Processing\"):\n",
    "            file_name = re.findall(r'\\d+\\w', file)\n",
    "    #         print(file_name)\n",
    "            try:\n",
    "                create_json_file(folder_path+file, outputPath+file+'.pkl')\n",
    "            except:\n",
    "                error_file.append(file)\n",
    "                continue\n",
    "        \n",
    "        print(error_file)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
