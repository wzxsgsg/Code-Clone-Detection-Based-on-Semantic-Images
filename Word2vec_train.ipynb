{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已完成： 1\n",
      "已完成： 2\n",
      "已完成： 3\n",
      "已完成： 4\n",
      "已完成： 5\n",
      "已完成： 6\n",
      "已完成： 7\n",
      "已完成： 8\n",
      "已完成： 9\n",
      "已完成： 10\n",
      "已完成： 11\n",
      "已完成： 12\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "\n",
    "stmt_list = ['MethodDeclaration', 'FormalParameter', 'LocalVariableDeclaration', 'VariableDeclarator', 'BinaryOperation',\n",
    "             'IfStatement', 'BlockStatement', 'StatementExpression', 'Assignment', 'MethodInvocation', 'ForStatement', 'TryStatement',\n",
    "             'ClassCreator', 'CatchClause', 'WhileStatement', 'ReturnStatement']\n",
    "\n",
    "word_bag = []\n",
    "\n",
    "# stmt_list = node_type_dict.keys()\n",
    "for index in range(12):\n",
    "    input_path = \"processed_data/GCJ/GCJ_json/\" + str(index+1) + '/'\n",
    "    files = os.listdir(input_path)\n",
    "\n",
    "    for file in files:\n",
    "        with open(input_path+file, 'rb') as file_1:\n",
    "            node_dict = pickle.load(file_1)\n",
    "\n",
    "        for k, v in node_dict.items():\n",
    "            node_sequence = v[1]\n",
    "            stmt_index = []\n",
    "\n",
    "            for i in range(len(node_sequence)):\n",
    "                if node_sequence[i] in stmt_list:\n",
    "                    stmt_index.append(i)\n",
    "\n",
    "            pairs = [[stmt_index[i], stmt_index[i+1]] for i in range(len(stmt_index)-1)]\n",
    "    #         print(stmt_index)\n",
    "    #         print(pairs[-1])\n",
    "            if len(stmt_index) >= 2:\n",
    "                if pairs[-1][1] != len(node_sequence):\n",
    "                    pairs.append([pairs[-1][1], len(node_sequence)])\n",
    "                for i in range(len(pairs)):\n",
    "                    if pairs[i][1] - pairs[i][0] <= 2 and i != len(pairs) - 1:\n",
    "                        pairs[i+1][0] = pairs[i][0]\n",
    "                    else:\n",
    "                        word_bag.append(node_sequence[pairs[i][0]:pairs[i][1]])\n",
    "            else:\n",
    "                word_bag.append(node_sequence)\n",
    "    \n",
    "    print(\"已完成：\", index + 1)\n",
    "\n",
    "model = Word2Vec(word_bag, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.train(word_bag, total_examples=1, epochs=10)\n",
    "\n",
    "# 保存模型\n",
    "model.save(\"models/word2vec_model_gcj.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c52d079e3248dca7f7b5f0346eaee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39d4304a0674758a41a11b5ed44a682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ccfde3a47b453f8dfb9a673bd8fef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31368551eaa1458faa3957f6b3b77e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b05feea1e9641b1b88457d054f798b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f4f01e9d6a841a29fe23e587e25c77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eadde96cacb4d4483265e3081a1061e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06895ea29fb540f4b8291986214997bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffed103d0f3418c836d63724f0cdcc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc7f2a707ec40dbb935eab1c6c99127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f99e467aab475aaae66e26b3a9b1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4ae9fb314c843ccb0ee699c506701fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "stmt_list = ['MethodDeclaration', 'FormalParameter', 'LocalVariableDeclaration', 'VariableDeclarator', 'BinaryOperation',\n",
    "             'IfStatement', 'BlockStatement', 'StatementExpression', 'Assignment', 'MethodInvocation', 'ForStatement', 'TryStatement',\n",
    "             'ClassCreator', 'CatchClause', 'WhileStatement', 'ReturnStatement']\n",
    "\n",
    "\n",
    "def random_walk(G, num_walks, walk_length):\n",
    "    # 随机游走生成节点序列\n",
    "    walks = []\n",
    "\n",
    "    for _ in range(num_walks):\n",
    "        for node in G.nodes():\n",
    "            walk = [str(node)]\n",
    "            current_node = node\n",
    "            for _ in range(walk_length - 1):\n",
    "                neighbors = list(G.neighbors(current_node))\n",
    "                if neighbors:\n",
    "                    next_node = np.random.choice(neighbors)\n",
    "                    walk.append(str(next_node))\n",
    "                    current_node = next_node\n",
    "                else:\n",
    "                    break\n",
    "            walks.append(walk)\n",
    "    return walks\n",
    "\n",
    "\n",
    "def compute_Embedding(path, walks_list):\n",
    "    with open(path, 'rb') as file_1:\n",
    "        node_dict = pickle.load(file_1)\n",
    "\n",
    "    for k, v in node_dict.items():\n",
    "        G = v[0]\n",
    "        walks = random_walk(G, num_walks=10, walk_length=4)\n",
    "        for walk in walks:\n",
    "            walks_list.append(walk)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     input_path = \"processed_data/node_pkl_subgraph/\"\n",
    "\n",
    "#     files = os.listdir(input_path)\n",
    "    \n",
    "#     print(len(files))\n",
    "#     walks_list = []   \n",
    "#     for index, file in tqdm(enumerate(files), total=len(files), desc=\"Processing\"):\n",
    "#         embeddings = compute_Embedding(input_path + file, walks_list)\n",
    "\n",
    "#     model = Word2Vec(walks_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "#     model.train(walks_list, total_examples=1, epochs=20)\n",
    "\n",
    "#     # 保存模型\n",
    "#     model.save(\"models/word2vec_model_random_walks.bin\")\n",
    "\n",
    "def main():\n",
    "    walks_list = []\n",
    "    for index in range(12):\n",
    "        input_path = \"processed_data/GCJ/GCJ_json/googlejam4_src/\" + str(index+1) + '/'\n",
    "        files = os.listdir(input_path)\n",
    "\n",
    "        print(len(files))\n",
    "           \n",
    "        for index, file in tqdm(enumerate(files), total=len(files), desc=\"Processing\"):\n",
    "            compute_Embedding(input_path + file, walks_list)\n",
    "\n",
    "    model = Word2Vec(walks_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    model.train(walks_list, total_examples=1, epochs=20)\n",
    "\n",
    "    # 保存模型\n",
    "    model.save(\"models/word2vec_model_random_walks_gcj.bin\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c04c47ff99b488bbf5f4d454bddb5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/478 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6173112c68fd4c6487ac3f9b5ccb3996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22277b474aba459c98c8bd8025efdf04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/242 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f2f3f1047a4739b0232ee6fdad6264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a70a4be8534551878bc427a57a24af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d407098b904e4b258c0bda5ae53b47e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/435 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29fc8dfcf69543afbc8404897095e3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00bc5a22f16548298aef88be818c8d4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a8b63e5b0c4925ab18bbd5958c2a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cbba5042b444639f87710fe30bfed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74db07efb9084ec09aa1ce971313f70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16c0c54cf0647ee982c35a8b64e3c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def compute_Embedding(path, sequence_list):\n",
    "    with open(path, 'rb') as file_1:\n",
    "        node_dict = pickle.load(file_1)\n",
    "\n",
    "    for k, v in node_dict.items():\n",
    "        sequence = v[1]\n",
    "        sequence_list.append(sequence)\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     input_path = \"processed_data/node_pkl_subgraph/\"\n",
    "\n",
    "#     files = os.listdir(input_path)\n",
    "    \n",
    "#     print(len(files))\n",
    "#     sequence_list = []   \n",
    "#     for index, file in tqdm(enumerate(files), total=len(files), desc=\"Processing\"):\n",
    "#         embeddings = compute_Embedding(input_path + file, sequence_list)\n",
    "\n",
    "#     model = Word2Vec(sequence_list, vector_size=1, window=5, min_count=1, workers=4)\n",
    "#     model.train(sequence_list, total_examples=1, epochs=20)\n",
    "\n",
    "#     # 保存模型\n",
    "#     model.save(\"models/word2vec_model_bcb_1.bin\")\n",
    "    \n",
    "\n",
    "def main():\n",
    "    sequence_list = []\n",
    "    for index in range(12):\n",
    "        input_path = \"processed_data/GCJ/GCJ_json/googlejam4_src/\" + str(index+1) + '/'\n",
    "        files = os.listdir(input_path)\n",
    "\n",
    "        print(len(files))\n",
    "           \n",
    "        for index, file in tqdm(enumerate(files), total=len(files), desc=\"Processing\"):\n",
    "            compute_Embedding(input_path + file, sequence_list)\n",
    "\n",
    "    model = Word2Vec(sequence_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    model.train(sequence_list, total_examples=1, epochs=20)\n",
    "\n",
    "        # 保存模型\n",
    "    model.save(\"models/word2vec_model_gcj.bin\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9133\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d928d57e6f3432db7ca4e3ae2e418ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/9133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import random\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def compute_Embedding(path, sequence_list):\n",
    "    with open(path, 'rb') as file_1:\n",
    "        node_dict = pickle.load(file_1)\n",
    "\n",
    "    for k, v in node_dict.items():\n",
    "        sequence = v[1]\n",
    "        sequence_list.append(sequence)\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_path = \"processed_data/node_pkl_subgraph/\"\n",
    "    input_path_1 = \"processed_data/gcd_1.pkl\"\n",
    "    input_path_2 = \"processed_data/gcd_2.pkl\"\n",
    "\n",
    "    files = os.listdir(input_path)\n",
    "    \n",
    "    print(len(files))\n",
    "    sequence_list = []   \n",
    "    for index, file in tqdm(enumerate(files), total=len(files), desc=\"Processing\"):\n",
    "        compute_Embedding(input_path + file, sequence_list)\n",
    "    \n",
    "    compute_Embedding(input_path_1, sequence_list)\n",
    "    compute_Embedding(input_path_2, sequence_list)\n",
    "    \n",
    "    model = Word2Vec(sequence_list, vector_size=1, window=5, min_count=1, workers=4)\n",
    "    model.train(sequence_list, total_examples=1, epochs=20)\n",
    "\n",
    "    # 保存模型\n",
    "    model.save(\"models/word2vec_model_bcb_1.bin\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
